# LLM Provider: "ollama" (default, local, private) or "groq" (cloud, opt-in)
LLM_PROVIDER=ollama

# Ollama config (local inference - data stays on your machine)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Groq config (cloud inference - WARNING: your data context is sent to Groq servers)
# Get a free API key at https://console.groq.com/keys
# Only set these if you're okay with sending data summaries to the cloud
# GROQ_API_KEY=gsk_your_key_here
# GROQ_MODEL=llama-3.1-8b-instant

CHROMA_PERSIST_DIR=./vectorstore
OUTPUT_DIR=./outputs
